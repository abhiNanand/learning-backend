Reading files with Node.js
---------------------------

The simplest way to read a file in Node.js is to use the fs.readFile() method, passing it the file path, encoding and a callback function that will be called with the file data.

import the built-in File System module
import fs form 'node:fs'

fs.readFile(path,(err,data)=>{});


Jab aap fs.readFile('/Users/joe/test.txt', 'utf8', (err, data) => {...}) likhte ho, iska matlab hai:

Aap computer ko keh rahe ho:
"Is file ko UTF-8 encoding ke hisaab se padho, taki text theek samajh aaye."

Agar aap encoding specify nahi karoge, toh computer confuse ho jayega ki har byte ka matlab kya hai, aur aapko garbled text mil sakta hai.

UTF-8 ek system hai jo characters ko numbers (aur phir bytes) mein convert karta hai.

ASCII encoding mein A = 65, B = 66, C = 67 hota hai, aur UTF-8 is ASCII ko bhi support karta hai.

Matlab UTF-8 mein agar simple English letters hain (A, B, C...), toh wo exactly same numbers assign karta hai jaise ASCII.

example.

1.
const fs = require('node:fs');

fs.readFile('note1.txt', 'utf8', (err, data) => {
  if (err) {
    console.log(err);
    return;
  }
  console.log(data);
});

//2.Alternatively, you can use the synchronous version fs.readFileSync():
const fs = require('node:fs');

try {
  const data = fs.readFileSync('/Users/joe/test.txt', 'utf8');
  console.log(data);
} catch (err) {
  console.error(err);
}

3.by using promise
import fs from 'node:fs/promises';

try {
  const data = await fs.readFile('/Users/joe/test.txt', { encoding: 'utf8' });
  console.log(data);
} catch (err) {
  console.error(err);
}



readFile Reads the entire file at once.
Blocks memory with the full contents.
Not scalable for very large files.

Thats why we use streams
Stream allows us to read file piece by piece chunk by chunk.

import fs from 'node:fs';

const readStream = fs.createReadStream('largefile.txt', { encoding: 'utf8' });

readStream.on('data', (chunk) => {
  console.log('Received chunk:', chunk);
});

readStream.on('end', () => {
  console.log('Finished reading file.');
});

readStream.on('error', (err) => {
  console.error('Error reading file:', err);
});
Key Benefits of Using Streams:
Memory-efficient: Only a small part of the file is in memory at any time.

Scalable: Handles very large files without memory pressure.

Non-blocking: Stream operations are asynchronous.










import fs from 'fs';
import { pipeline } from 'node:stream/promises';
import path from 'path';

const fileUrl = 'https://www.gutenberg.org/files/2701/2701-0.txt';
const outputFilePath = path.join(process.cwd(), 'moby.md');

async function downloadFile(url, outputPath) {
  const response = await fetch(url);

  if (!response.ok || !response.body) {
    throw new Error(`Failed to fetch ${url}. Status: ${response.status}`);
  }

  const fileStream = fs.createWriteStream(outputPath);
  console.log(`Downloading file from ${url} to ${outputPath}`);

  await pipeline(response.body, fileStream);
  console.log('File downloaded successfully');
}

async function readFile(filePath) {
  const readStream = fs.createReadStream(filePath, { encoding: 'utf8' });

  try {
    for await (const chunk of readStream) {
      console.log('--- File chunk start ---');
      console.log(chunk);
      console.log('--- File chunk end ---');
    }
    console.log('Finished reading the file.');
  } catch (error) {
    console.error(`Error reading file: ${error.message}`);
  }
}

try {
  await downloadFile(fileUrl, outputFilePath);
  await readFile(outputFilePath);
} catch (error) {
  console.error(`Error: ${error.message}`);
}
